{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_json('./data/review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function to get tokens from document\n",
    "def tokenize(doc):\n",
    "    tokens = []\n",
    "    doc = nlp(doc)\n",
    "    for token in doc:\n",
    "        if((token.is_punct==False)&(token.is_stop==False)&(token.is_alpha==True)):\n",
    "           tokens.append(token.text.lower())\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beware',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'small',\n",
       " 'business',\n",
       " 'los',\n",
       " 'alamitos',\n",
       " 'received',\n",
       " 'looked',\n",
       " 'like',\n",
       " 'legitimate',\n",
       " 'bill',\n",
       " 'account',\n",
       " 'number',\n",
       " 'called',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'listed',\n",
       " 'wait',\n",
       " 'time',\n",
       " 'hold',\n",
       " 'said',\n",
       " 'minutes',\n",
       " 'leave',\n",
       " 'message',\n",
       " 'live',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'matter',\n",
       " 'number',\n",
       " 'selected',\n",
       " 'left',\n",
       " 'firm',\n",
       " 'message',\n",
       " 'contacting',\n",
       " 'bbb',\n",
       " 'attorney',\n",
       " 'company',\n",
       " 'trying',\n",
       " 'scam',\n",
       " 'businesses',\n",
       " 'illegal']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the tokenise fucntion by sending the first yelp review as a document\n",
    "tokenize(yelp['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the spacy vectors to create vectorisations\n",
    "def get_vectors(doc):\n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    return doc.vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpvetors = [get_vectors(review) for review in yelp['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "yelpvetors = np.array(yelpvetors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelpvetors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review = \"I went to the restuarant in the lunch rush hour and there was no space to wait. \\\n",
    "                The manager was very rude and asked me to come back after 30 minutes but he refused waitlist me.\\\n",
    "                Finally when i placed the order, the food i got was half burnt.\\\n",
    "                  Don't go to this establisment. Service is poor  and food quality is bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=None, n_neighbors=10, p=2, radius=1.0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "near_near = NearestNeighbors(10)\n",
    "near_near.fit(yelpvetors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.53463319, 0.54520934, 0.5469089 , 0.54805095, 0.55250154,\n",
       "         0.56342904, 0.56857038, 0.57167312, 0.5746451 , 0.57670553]]),\n",
       " array([[2394, 8831, 9381, 8865, 8786,  358, 4854, 5858, 5516, 3104]]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the nearest neigbours to the fake reivew\n",
    "neighbours = near_near.kneighbors(np.array(get_vectors(fake_review)).reshape(1,-1))\n",
    "neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I was at this Opa today and it sucked!  Here\\'s how it all played out folks. I went with 3 coworkers for lunch. We got to the mall at 1130 so that we could avoid the lunch rush. I had Opa on the weekend at a different location and it was so good I thought i would go back. As a vegetarian, the falafel option is a good one for me.  There was a girl behind the counter who looked like she should have been in  \"Night of the Living Dead\" eyes glazed over, completely ignorant to the fact that myself and someone else were waiting.  She was making a huge bowl of Salad ( I guess getting ready for the lunch rush I was trying to avoid).  A few minutes later ( literally) another staff member came out and was putting frozen kabobs of chicken and beef on the grill. She then stopped and took the guys order in front of me....then about 4 minutes later she got to me, another 5 minutes I was getting my food.  By that time the line was huge but that didn\\'t affect the zombies behind the counter, they just kept continued to move at the same pace.  \\nBy the time I got to the table my coworkers were done! They had gotten thier food and eaten it in the time it took me to get mine.  Worst of all the greek salad was bland, watery and lacked any zing at all. I pitched that in the garbage. The falafel was good. \\n I can handle waiting, I can handle alot from service people if they are pleasant and acknowledge thier customers. This whole experience just sucked. I can\\'t even say bad service, because essentially i was served by two robots, devoid of any interaction, work ethic, or warmth.  OPA! I dont think so..... i wont be back to this location.'\n",
      " 'Been to this location twice.  First time I waited forever in the drive thru for a mini $1.49 sandwich.   Second time I went we went inside and once again waited longer than usual for fast food.  There was someone complaining about a sandwich to a disinterested manager.  I felt the staff was not well trained when asking questions about different sandwiches and when they asked the manager he seems bothered.  I do not intend to return to this location.'\n",
      " 'I know this is Bob Evan\\'s. I\\'d be comparing to the restaurants alike (e.g. Eat N\\' Park, Denny\\'s etc.)  \\nI used to enjoy their breakfast. Their sausage is solid, the biscuits are great.  If I had to pick a chain restaurant to eat breakfast on the road, Bob Evan\\'s was my go to.  But this location kind of ruined it for me. The worst service ever.  I was sat down immediately, but had to wait about 10min for a waitress to approach our table to see if we wanted a drink.  Food was cold by the time it showed up.  But i don\\'t dare send any food back at a restaurant like this.  So I suffered through the cold brakfast.  Well, the waitress never came back and see how our food was.  And unfortunately this wasn\\'t an isolated incident.  Another time, I was not feeling well, and wanted to pick up some chicken noodle soup to go.  I walked in, and stood there for 5-7 min before I finally grabbed one of the waitresses and asked \"excuse me, does anyone work here????\"\\nI don\\'t expect too much from Bob Evan\\'s, but this one is one of the worst when it comes to the services.  If you want breakfast, go across the street to Central Diner. Much better food with excellent service.'\n",
      " \"The Griddle Cafe is one of my favorite Cali spots.  I go almost every trip.  I was so excited that they were coming to Vegas.  We waited a few weeks for the crowds to die down.  Well they died down alright.  The first time we went...they were closed.  We asked security nearby and they said they shortened their hours to 11-4 during the week and 24 hrs only on the weekend.  He said they were slow at night.  I was expecting a line.  That didn't seem like a good sign.  Less then a month and already cutting back on hours.  Is the SLS that dead?  Damn.  Turns out it is 11pm-4pm.  So not near as bad as 11am-4pm we took it as.  I thought I was never going to get to eat here.  Finally we venture back, I got Red Velvet PanCAKE and hubby got a Shrimp BLT.  Same quality, taste and huge portions as their LA counterpart.  Our food was great but service could have been better.  Waiter wasn't the best and then kept the change without asking.  That was kinda weird to just assume I'm gonna give you a 34% tip on your shitty service.  I am a fan of Griddle Cafe and food was just as good as in LA so I will be back and hopefully the service was a fluke this time.\"\n",
      " \"Extreme bad service.  5 of us went on a Sunday afternoon.  The service went from bad to worse.  We had a drink and were waiting for the server to come back to order refills 10 minutes later she appeared.  One of the orders was beer.  15 minutes later she appeared to inform us that the keg needed to be replaced.  We got our food and had no utensils and no server.  The manager was too busy leaning over the counter to pay any attention to our problems  We asked another server and all she said was I'll find yours.  Helpful...still didn't see her.  Sorry can't recommend this place.\"\n",
      " 'If it could be 0 stars it would.  I wish I could review the food or the drinks, but the bald, rude host, made sure it wouldn\\'t happen.  It was a Thursday evening, the restaurant had a 12 top, with their food and a few 2 tops.  The bar was almost completely open.  We waited at the Host stand for a bit, when approached by the Host, he very quickly let us know the restaurant was way too busy for us to sit down and eat.  When I asked for clarification on \"busy\", he simply stated we could have to-go food immediately, but if we sat down, it would be at least an hour.  There was no way it would have been an hour, so I elected to be sat.  He refused to seat us.  He then stated our only option was to-go food, right away.  I asked to sit at the bar.  He said there was no room.  How could you cook our to-go food immediately but to eat the food in the restaurant would take over an hour?!  I am still looking for the logic in that.  I have worked in the industry for YEARS.  I have never experienced such extreme prejudice or whatever it was he was giving us.  Nor have I ever been turned away from a restaurant.  I had been looking forward to dining here for a very, long time.  I do not think I will be able to bring myself back to attempt to be offended again.  The only reason I want to, is to tell the rude Host how horrible I still think he is!  It\\'s too bad.  I will continue to share my horrible experience, as it definitely ranks as the worst restaurant experience I have EVER had.  & we dine out, often.  At lots of places.  Fine dining to neighborhood bars.  No one has EVER been that rude with absolutely zero reason.  Shame on him and shame on Harley\\'s for employing people who treat potential new customers like mud.'\n",
      " 'Terrible ! Terrible ! Terrible! \\nLet\\'s start by saying the old female manager with the weird lips is a complete as*hole! The server couldn\\'t even bring out the beverages correct .. I ordered soda water with lime! She delivered tap water with lemon .. 10 minutes later. great!\\nMy friend and I ordered our food and it literally took 25 minutes to come out. Once it did we had to ask for silverware. She dropped that off and walked away. Took one bite of food and it was cold. Had no heat at all. After sitting there for another 15 minutes with no service, I asked the next server I seen for the manager who came 5 minutes later. I explained to her how the food is cold and we\\'ve been sitting trying to wait but the server never came back so we began to eat, being that we\\'ve been here for quite some time. She tells me it\\'s nothing she can do besides reheat the food. \\nUmm that would have been fine 15 minutes ago if SOMEBODY did a table touch like they was suppose to! However, I\\'m a server and have been for years and there is absolutely nothing acceptable about my service nor should I be forced to pay full price for my frozen food. Anyways, she brings the check and I proceed to the cashier to pay and of course it\\'s little miss sunshine, lady manager.. I pay and obviously not happy about it and this witch had the nerve to tell me to never come back & she tells \"security\" to escort us out as if I\\'ve done something! Like Susan.. chill! Don\\'t try to profile me! I wouldn\\'t step foot back in that stanky, roach coach, awkward smelling, no service having, everybody in the sunken place establishment ever again! Y\\'all better tighten up cause phones, social media & dirty dining is something special, ain\\'t it..\\nGood day'\n",
      " \"I love the theme and the hotel is very up beat!! The room was better than I expected, a big plus! \\n\\nMy bf, a friend and I were walking here from venetian and got a little lost since PH restaurant and hotel are in two separate places. That was very inconvenient and misleading. Doesn't make sense to have those 2 separated. \\n\\nThe customer service was ok. Could have been way better. I wanted to order room service since I was exhausted from the wkend and the dine in menu was missing in our room. We called for a menu and nothing half an hr later. Called again and they tell us it's in the room book. Told her it wasn't. Obviously, I wouldn't call if we had one. Getting cranky from waiting and starving. Was not a happy camper from then. Didn't order room service because they took too long so we headed out to find food.\"\n",
      " \"This is the sushi restaurant to come to if you want to overpay and feel like an unwelcomed guest.\\n\\nMy girlfriend and I were in a hurry so we stopped by here without a reservation on a Saturday night. Three (!) hostesses (no idea why they needed that many since none of them were doing anything) at the stand rudely told us all we could sit at would be the sushi bar even though we saw many open tables. I did not complain and happily accepted that. Big mistake.\\n\\nWe were jammed into two spots at the tight sushi bar and given a drink list. After waiting about 10 minutes for someone to take our drink bar, the sushi chef impatiently asked if we were ready. Um, no menus yet. He yelled at a server who came by and apologized and gave us menus and took our drink orders. Ok, honest mistake we thought...\\n\\nDrinks came back and we ordered edamame and our sushi. Edamame never came and we couldn't get our server back until after we ate our rolls, which did come out quick. \\n\\nI complimented our sushi chef on the rainbow roll and he barely grunted back at me. Meanwhile, the chef to our left was having a very long conversation with the couple next to us. Maybe we got a grumpy chef? Either way, the rolls were good but I felt as if I had done something to piss off the staff which I felt to be pretty impossible since we had barely even got anyone to talk to us and I had been nothing but polite and complimentary.\\n\\nWhen asked about the edamame, server just smacked his head and said sorry. He was nice enough but apparently overworked or not prepared to handle weekend volume. Either way, pretty poor from what I expected to be some of the best sushi on the strip.\\n\\nBill came to $70 for 3 rolls, salmon sashimi and a soft drink or two. Couldn't wait to get out of here. Food was good but I'd rather have my money back. This is the worst side of Vegas: overpriced, rude service, and no one pushing the staff to do better.\"\n",
      " \"I went here as a new patient.  They open at 9am and their first appt. time is 9:30am.  My appt. was for 10am and came early to fill out paperwork.  I sat in the waiting room for 40 minutes before being taken back to a room.  The attendant told me to go to the 3rd room on the right and then didn't come in the room after me.  I sat there another 5 minutes before the attendant came in to inform me that they were going to take x-rays.  I said that was fine and he left again.  Another 10 minutes went by and still nothing had been done with me.  I left a little under an hour after my appointment should have started because I was sick of waiting and STILL nobody had so much as talked to me.  The walls had scratches all along them and the chair was wet when I sat down in it.  Presumably it was just wiped down with a cleaning agent but at least dry the chair off before you send someone to sit in it.  All around a bad experience.  I would not recommend this place to ANYONE!\"]\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 similar review from yelp which are similar to fake review\n",
    "for index in neighbours[1]:\n",
    "    print(yelp.iloc[index]['text'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a piepline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function to get lemmas of review to input for TfidVector\n",
    "def get_lemmas(df):\n",
    "    ncks = []\n",
    "    for doc in df:\n",
    "        doc = nlp(doc)\n",
    "        tokens = \"\"\n",
    "        for token in doc:\n",
    "            if((token.is_stop==False) & (token.is_punct==False)&(token.is_alpha==True)):\n",
    "                tokens += token.lemma_ + \" \"\n",
    "        ncks.append(tokens)\n",
    "    return ncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting lemmas from the yelp reviews\n",
    "yelp['lemmas'] = get_lemmas(yelp['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    beware fake fake fake small business Los Alami...\n",
       "1    come lunch Togo service quick staff friendly c...\n",
       "2    Vegas dozen time step foot Circus Circus reaso...\n",
       "3    go night close street party good actually grou...\n",
       "4    star bad price lunch senior pay eat hot food s...\n",
       "Name: lemmas, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['lemmas'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('tfid',TfidfVectorizer(stop_words='english',min_df=0.025,max_df=0.95,max_features=500)),\n",
    "        ('rfc',RandomForestClassifier(n_estimators=100))\n",
    "        ])\n",
    "\n",
    "\n",
    "parameters = { \n",
    "    'vect__max_df': ( 0.75,0.95),\n",
    "    'vect__min_df': (.025,0.05),\n",
    "    'vect__max_features': (500,1000),\n",
    "    'clf__n_estimators':(10,20),\n",
    "    'clf__max_depth':(3,5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commenting the GridSearch code , as the notebook is crashing due to memory allocation problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Using grid search to fit for the best paramaeters\n",
    "# grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "# grid_search.fit(yelp.lemmas, yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfid', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=500, min_df=0.025,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the data to the pipeline\n",
    "pipe.fit(yelp.lemmas,yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting the star for the fake review\n",
    "pipe.predict(get_lemmas([fake_review]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Keep the `iterations` parameter at or below 5 to reduce run time\n",
    "    - The `workers` parameter should match the number of physical cores on your machine.\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the vocubalary of the yelp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7f0f26b29450>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = Dictionary(yelp['lemmas'].str.split())\n",
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(530, 1), (680, 1), (3994, 1), (25333, 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the dictionary created\n",
    "id2word.doc2bow(tokenize(\"This is a sample yelp review which i am writing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25767"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in yelp['lemmas'].str.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your LDA model should be ready for estimation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   iterations=5,\n",
    "                   workers=2,\n",
    "                   num_topics = 10 # You can change this parameter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"place\" + 0.012*\"good\" + 0.011*\"time\" + 0.009*\"great\" + 0.009*\"food\" + 0.008*\"come\" + 0.007*\"go\" + 0.007*\"order\" + 0.006*\"like\" + 0.005*\"get\"'),\n",
       " (1,\n",
       "  '0.012*\"good\" + 0.011*\"food\" + 0.009*\"service\" + 0.008*\"get\" + 0.008*\"come\" + 0.008*\"place\" + 0.008*\"try\" + 0.007*\"time\" + 0.007*\"like\" + 0.007*\"great\"'),\n",
       " (2,\n",
       "  '0.014*\"good\" + 0.010*\"food\" + 0.009*\"place\" + 0.009*\"service\" + 0.008*\"order\" + 0.008*\"come\" + 0.008*\"time\" + 0.007*\"great\" + 0.007*\"like\" + 0.006*\"go\"'),\n",
       " (3,\n",
       "  '0.018*\"good\" + 0.012*\"place\" + 0.009*\"food\" + 0.008*\"like\" + 0.007*\"service\" + 0.007*\"great\" + 0.007*\"time\" + 0.006*\"come\" + 0.006*\"order\" + 0.006*\"go\"'),\n",
       " (4,\n",
       "  '0.012*\"good\" + 0.012*\"great\" + 0.011*\"time\" + 0.009*\"place\" + 0.009*\"food\" + 0.009*\"like\" + 0.007*\"service\" + 0.006*\"come\" + 0.005*\"get\" + 0.005*\"look\"'),\n",
       " (5,\n",
       "  '0.011*\"place\" + 0.011*\"good\" + 0.011*\"food\" + 0.010*\"time\" + 0.007*\"great\" + 0.006*\"come\" + 0.006*\"get\" + 0.006*\"like\" + 0.006*\"go\" + 0.006*\"service\"'),\n",
       " (6,\n",
       "  '0.013*\"good\" + 0.011*\"place\" + 0.011*\"great\" + 0.010*\"like\" + 0.009*\"come\" + 0.009*\"food\" + 0.009*\"order\" + 0.009*\"time\" + 0.008*\"go\" + 0.008*\"service\"'),\n",
       " (7,\n",
       "  '0.013*\"good\" + 0.010*\"food\" + 0.010*\"come\" + 0.009*\"great\" + 0.008*\"time\" + 0.008*\"service\" + 0.008*\"order\" + 0.007*\"place\" + 0.007*\"like\" + 0.007*\"go\"'),\n",
       " (8,\n",
       "  '0.014*\"place\" + 0.013*\"good\" + 0.011*\"food\" + 0.010*\"great\" + 0.010*\"time\" + 0.008*\"like\" + 0.008*\"order\" + 0.007*\"come\" + 0.007*\"service\" + 0.007*\"get\"'),\n",
       " (9,\n",
       "  '0.013*\"place\" + 0.010*\"good\" + 0.009*\"like\" + 0.009*\"food\" + 0.009*\"come\" + 0.007*\"get\" + 0.007*\"order\" + 0.007*\"service\" + 0.007*\"great\" + 0.006*\"go\"')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place good time great food come go order like get\n",
      "\n",
      "\n",
      "good food service get come place try time like great\n",
      "\n",
      "\n",
      "good food place service order come time great like go\n",
      "\n",
      "\n",
      "good place food like service great time come order go\n",
      "\n",
      "\n",
      "good great time place food like service come get look\n",
      "\n",
      "\n",
      "place good food time great come get like go service\n",
      "\n",
      "\n",
      "good place great like come food order time go service\n",
      "\n",
      "\n",
      "good food come great time service order place like go\n",
      "\n",
      "\n",
      "place good food great time like order come service get\n",
      "\n",
      "\n",
      "place good like food come get order service great go\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in lda.print_topics()]\n",
    "topics = [' '.join(t) for t in words]\n",
    "for t in topics: \n",
    "    print(t)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "alltopics = [topic.split() for topic in topics]\n",
    "topicslist = [item for sublist in alltopics for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "counttopics = Counter(topicslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsdf = pd.DataFrame(counttopics.keys(),counttopics.values()).reset_index()\n",
    "topicsdf.columns = ['no_of_items','topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#showing the visualisation for top 10 topics in the yelp reviews\n",
    "import squarify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "squarify.plot(sizes=topicsdf['no_of_items'], label=topicsdf['topic'], alpha=.8 )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation on Topics from using LDA\n",
    "\n",
    "As we can see from the above plot, the important topics in any of the yelp reviews is **Good Food*, **Good Place** and further topics such as **great food**, **on time**, **order**\n",
    "\n",
    "We know from observation that yelp is about rating the business establishments especially food joints and restuarants\n",
    "\n",
    "But using LDA, the model after looking at the yelp revies, it has identified that the reviews are about food and the places of food. This indicates to us the model was able to identify the reviews correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify Business Entities from Yelp Reviews\n",
    "\n",
    "rowent = []\n",
    "for doc in yelp['text']:\n",
    "    doc = nlp(doc)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ in ['GPE','PERSON','ORG']):\n",
    "            entities.append(ent)\n",
    "    rowent.append(entities)\n",
    "yelp['entities'] = rowent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        [(Los, Alamitos), (CA), (BBB)]\n",
       "1                                              [(Togo)]\n",
       "2     [(Vegas), (Circus, Circus), (Vegas), (Circus, ...\n",
       "3                                          [(Facebook)]\n",
       "4                                                    []\n",
       "5                                     [(Taco, Tuesday)]\n",
       "6                                [(Airy), (Las, Vegas)]\n",
       "7                                                    []\n",
       "8                                   [(Charlotte), (Nc)]\n",
       "9                                          [(Stations)]\n",
       "10                                  [(April, Bambina)]\n",
       "11                                  [(Thw), (AC), (AC)]\n",
       "12                                                   []\n",
       "13                                                   []\n",
       "14           [(Bryan), (Bryan), (ROC), (Goettl), (ARS)]\n",
       "15                                          [(Buffalo)]\n",
       "16           [(Charlotte), (UGA), (Louisville), (T.V.)]\n",
       "17                               [(Montreal), (Audrey)]\n",
       "18    [(Groupon), (Maryann), (Maryann), (Groupon), (...\n",
       "19    [(Oak), (Las, vegas), (Mules), (SnowStorm), (P...\n",
       "Name: entities, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for the entities identified \n",
    "yelp['entities'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Vectorisation for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Vectorisation , we can use CountVectoriser , TFIDVectoriser, Vectors from Spacy \n",
    "\n",
    "The main difference between CountVectoriser and TFIDVectoriser is that CountVectoriser doesn't take into account the length of the document into consideration, due to this a lengthy document in comparison to a short document will be given higher weightage\n",
    "\n",
    "This problem is rectified by TFIDVectoriser by looking for commonality of words across all documents and then looking for unique words in a single document which differentiate it from other documents. It also normalises the count between 0 and 1\n",
    "\n",
    "The Advantage of Using Vectorisation from spacy is that , spacy creates vector based on context of the word in comparison to the document. It is able to do that as it has trained its words against a large collection of documents and usign the vectorisation from spacy will also preserve the context of words in the document"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.14.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
